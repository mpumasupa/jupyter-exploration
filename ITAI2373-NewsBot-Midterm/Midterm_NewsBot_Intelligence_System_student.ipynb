{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ NewsBot Intelligence System\n",
    "## ITAI 2373 - Mid-Term Group Project Template\n",
    "\n",
    "**Team Members:** [Add your names here]\n",
    "**Date:** [Add date]\n",
    "**GitHub Repository:** [Add your repo URL here]\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Project Overview\n",
    "\n",
    "Welcome to your NewsBot Intelligence System! This notebook will guide you through building a comprehensive NLP system that:\n",
    "\n",
    "- üì∞ **Processes** news articles with advanced text cleaning\n",
    "- üè∑Ô∏è **Classifies** articles into categories (Politics, Sports, Technology, Business, Entertainment, Health)\n",
    "- üîç **Extracts** named entities (people, organizations, locations, dates, money)\n",
    "- üòä **Analyzes** sentiment and emotional tone\n",
    "- üìä **Generates** insights for business intelligence\n",
    "\n",
    "### üìö Module Integration Checklist\n",
    "- [ ] **Module 1:** NLP applications and real-world context\n",
    "- [ ] **Module 2:** Text preprocessing pipeline\n",
    "- [ ] **Module 3:** TF-IDF feature extraction\n",
    "- [ ] **Module 4:** POS tagging analysis\n",
    "- [ ] **Module 5:** Syntax parsing and semantic analysis\n",
    "- [ ] **Module 6:** Sentiment and emotion analysis\n",
    "- [ ] **Module 7:** Text classification system\n",
    "- [ ] **Module 8:** Named Entity Recognition\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Setup and Installation\n",
    "\n",
    "Let's start by installing and importing all the libraries we'll need for our NewsBot system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required packages (run this cell first!)\n",
    "!pip install spacy scikit-learn nltk pandas matplotlib seaborn wordcloud plotly\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "# Download NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "print(\"‚úÖ All packages installed successfully!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import all necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLP Libraries\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "# Scikit-learn for machine learning\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üìö All libraries imported successfully!\")\n",
    "print(f\"üîß spaCy model loaded: {nlp.meta['name']} v{nlp.meta['version']}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Data Loading and Exploration\n",
    "\n",
    "### üéØ Module 1: Understanding Our NLP Application\n",
    "\n",
    "Before we dive into the technical implementation, let's understand the real-world context of our NewsBot Intelligence System. This system addresses several business needs:\n",
    "\n",
    "1. **Media Monitoring:** Automatically categorize and track news coverage\n",
    "2. **Business Intelligence:** Extract key entities and sentiment trends\n",
    "3. **Content Management:** Organize large volumes of news content\n",
    "4. **Market Research:** Understand public sentiment about topics and entities\n",
    "\n",
    "**üí° Discussion Question:** What other real-world applications can you think of for this type of system? Consider different industries and use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load your dataset\n",
    "# üí° TIP: If using the provided dataset, upload it to Colab first\n",
    "# üí° TIP: You can also use sample datasets like BBC News or 20 Newsgroups\n",
    "\n",
    "# Option 1: Load provided dataset\n",
    "# df = pd.read_csv('news_dataset.csv')\n",
    "\n",
    "# Option 2: Load BBC News dataset (if using alternative)\n",
    "# You can download this from: https://www.kaggle.com/c/learn-ai-bbc/data\n",
    "\n",
    "# Option 3: Create sample data for testing (remove this when you have real data)\n",
    "sample_data = {\n",
    "    'article_id': range(1, 11),\n",
    "    'title': [\n",
    "        'Apple Inc. Reports Record Quarterly Earnings',\n",
    "        'Manchester United Defeats Chelsea 3-1',\n",
    "        'New AI Technology Revolutionizes Healthcare',\n",
    "        'President Biden Announces Climate Initiative',\n",
    "        'Netflix Releases New Original Series',\n",
    "        'Tesla Stock Rises After Production Update',\n",
    "        'Olympic Games Begin in Paris',\n",
    "        'Google Launches New Search Algorithm',\n",
    "        'Congress Passes Infrastructure Bill',\n",
    "        'Disney+ Subscriber Numbers Grow'\n",
    "    ],\n",
    "    'content': [\n",
    "        'Apple Inc. announced record quarterly earnings today, with CEO Tim Cook highlighting strong iPhone sales and services revenue growth.',\n",
    "        'Manchester United secured a convincing 3-1 victory over Chelsea at Old Trafford, with goals from Marcus Rashford and Bruno Fernandes.',\n",
    "        'A breakthrough AI system developed by researchers at Stanford University shows promise for early disease detection in medical imaging.',\n",
    "        'President Joe Biden unveiled a comprehensive climate change initiative aimed at reducing carbon emissions by 50% over the next decade.',\n",
    "        'Netflix premiered its latest original series to critical acclaim, featuring an ensemble cast and innovative storytelling techniques.',\n",
    "        'Tesla shares jumped 8% in after-hours trading following the company\\'s announcement of increased production capacity at its Texas facility.',\n",
    "        'The 2024 Olympic Games officially began in Paris with a spectacular opening ceremony attended by world leaders and celebrities.',\n",
    "        'Google introduced a new search algorithm that promises more accurate and contextually relevant results for users worldwide.',\n",
    "        'The U.S. Congress passed a bipartisan infrastructure bill allocating $1.2 trillion for roads, bridges, and broadband expansion.',\n",
    "        'Disney+ reported strong subscriber growth in Q3, reaching 150 million subscribers globally across all markets.'\n",
    "    ],\n",
    "    'category': ['Business', 'Sports', 'Technology', 'Politics', 'Entertainment', \n",
    "                'Business', 'Sports', 'Technology', 'Politics', 'Entertainment'],\n",
    "    'date': ['2024-01-15'] * 10,\n",
    "    'source': ['TechNews', 'SportsTimes', 'TechDaily', 'PoliticsToday', 'EntertainmentWeekly',\n",
    "              'BusinessWire', 'SportsCentral', 'TechReview', 'NewsNow', 'ShowBiz']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(sample_data)\n",
    "\n",
    "print(f\"üìä Dataset loaded successfully!\")\n",
    "print(f\"üìà Shape: {df.shape}\")\n",
    "print(f\"üìã Columns: {list(df.columns)}\")\n",
    "\n",
    "# Display first few rows\n",
    "df.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Basic dataset exploration\n",
    "print(\"üìä DATASET OVERVIEW\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total articles: {len(df)}\")\n",
    "print(f\"Unique categories: {df['category'].nunique()}\")\n",
    "print(f\"Categories: {df['category'].unique().tolist()}\")\n",
    "print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"Unique sources: {df['source'].nunique()}\")\n",
    "\n",
    "print(\"\\nüìà CATEGORY DISTRIBUTION\")\n",
    "print(\"=\" * 50)\n",
    "category_counts = df['category'].value_counts()\n",
    "print(category_counts)\n",
    "\n",
    "# Visualize category distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df, x='category', order=category_counts.index)\n",
    "plt.title('Distribution of News Categories')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# üí° STUDENT TASK: Add your own exploratory analysis here\n",
    "# - Check for missing values\n",
    "# - Analyze text length distribution\n",
    "# - Examine source distribution\n",
    "# - Look for any data quality issues"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Text Preprocessing Pipeline\n",
    "\n",
    "### üéØ Module 2: Advanced Text Preprocessing\n",
    "\n",
    "Now we'll implement a comprehensive text preprocessing pipeline that cleans and normalizes our news articles. This is crucial for all downstream NLP tasks.\n",
    "\n",
    "**Key Preprocessing Steps:**\n",
    "1. **Text Cleaning:** Remove HTML, URLs, special characters\n",
    "2. **Tokenization:** Split text into individual words\n",
    "3. **Normalization:** Convert to lowercase, handle contractions\n",
    "4. **Stop Word Removal:** Remove common words that don't carry meaning\n",
    "5. **Lemmatization:** Reduce words to their base form\n",
    "\n",
    "**üí° Think About:** Why is preprocessing so important? What happens if we skip these steps?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize preprocessing tools\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Comprehensive text cleaning function\n",
    "    \n",
    "    üí° TIP: This function should handle:\n",
    "    - HTML tags and entities\n",
    "    - URLs and email addresses\n",
    "    - Special characters and numbers\n",
    "    - Extra whitespace\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string and lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # üöÄ YOUR CODE HERE: Implement text cleaning\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Remove special characters and digits (keep only letters and spaces)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def preprocess_text(text, remove_stopwords=True, lemmatize=True):\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline\n",
    "    \n",
    "    üí° TIP: This function should:\n",
    "    - Clean the text\n",
    "    - Tokenize into words\n",
    "    - Remove stop words (optional)\n",
    "    - Lemmatize words (optional)\n",
    "    - Return processed text\n",
    "    \"\"\"\n",
    "    # Clean text\n",
    "    text = clean_text(text)\n",
    "    \n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # üöÄ YOUR CODE HERE: Implement tokenization and preprocessing\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words if requested\n",
    "    if remove_stopwords:\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatize if requested\n",
    "    if lemmatize:\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Filter out very short words\n",
    "    tokens = [token for token in tokens if len(token) > 2]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Test the preprocessing function\n",
    "sample_text = \"Apple Inc. announced record quarterly earnings today! Visit https://apple.com for more info. #TechNews\"\n",
    "print(\"Original text:\")\n",
    "print(sample_text)\n",
    "print(\"\\nCleaned text:\")\n",
    "print(clean_text(sample_text))\n",
    "print(\"\\nFully preprocessed text:\")\n",
    "print(preprocess_text(sample_text))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Apply preprocessing to the dataset\n",
    "print(\"üßπ Preprocessing all articles...\")\n",
    "\n",
    "# Create new columns for processed text\n",
    "df['title_clean'] = df['title'].apply(clean_text)\n",
    "df['content_clean'] = df['content'].apply(clean_text)\n",
    "df['title_processed'] = df['title'].apply(preprocess_text)\n",
    "df['content_processed'] = df['content'].apply(preprocess_text)\n",
    "\n",
    "# Combine title and content for full article analysis\n",
    "df['full_text'] = df['title'] + ' ' + df['content']\n",
    "df['full_text_processed'] = df['full_text'].apply(preprocess_text)\n",
    "\n",
    "print(\"‚úÖ Preprocessing complete!\")\n",
    "\n",
    "# Show before and after examples\n",
    "print(\"\\nüìù BEFORE AND AFTER EXAMPLES\")\n",
    "print(\"=\" * 60)\n",
    "for i in range(min(3, len(df))):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Original: {df.iloc[i]['full_text'][:100]}...\")\n",
    "    print(f\"Processed: {df.iloc[i]['full_text_processed'][:100]}...\")\n",
    "\n",
    "# üí° STUDENT TASK: Analyze the preprocessing results\n",
    "# - Calculate average text length before and after\n",
    "# - Count unique words before and after\n",
    "# - Identify the most common words after preprocessing"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Feature Extraction and Statistical Analysis\n",
    "\n",
    "### üéØ Module 3: TF-IDF Analysis\n",
    "\n",
    "Now we'll extract numerical features from our text using TF-IDF (Term Frequency-Inverse Document Frequency). This technique helps us identify the most important words in each document and across the entire corpus.\n",
    "\n",
    "**TF-IDF Key Concepts:**\n",
    "- **Term Frequency (TF):** How often a word appears in a document\n",
    "- **Inverse Document Frequency (IDF):** How rare a word is across all documents\n",
    "- **TF-IDF Score:** TF √ó IDF - balances frequency with uniqueness\n",
    "\n",
    "**üí° Business Value:** TF-IDF helps us identify the most distinctive and important terms for each news category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create TF-IDF vectorizer\n",
    "# üí° TIP: Experiment with different parameters:\n",
    "# - max_features: limit vocabulary size\n",
    "# - ngram_range: include phrases (1,1) for words, (1,2) for words+bigrams\n",
    "# - min_df: ignore terms that appear in less than min_df documents\n",
    "# - max_df: ignore terms that appear in more than max_df fraction of documents\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,  # Limit vocabulary for computational efficiency\n",
    "    ngram_range=(1, 2),  # Include unigrams and bigrams\n",
    "    min_df=2,  # Ignore terms that appear in less than 2 documents\n",
    "    max_df=0.8  # Ignore terms that appear in more than 80% of documents\n",
    ")\n",
    "\n",
    "# Fit and transform the processed text\n",
    "print(\"üî¢ Creating TF-IDF features...\")\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['full_text_processed'])\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"‚úÖ TF-IDF matrix created!\")\n",
    "print(f\"üìä Shape: {tfidf_matrix.shape}\")\n",
    "print(f\"üìù Vocabulary size: {len(feature_names)}\")\n",
    "print(f\"üî¢ Sparsity: {(1 - tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1])) * 100:.2f}%\")\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "tfidf_df['category'] = df['category'].values\n",
    "\n",
    "print(\"\\nüîç Sample TF-IDF features:\")\n",
    "print(tfidf_df.iloc[:3, :10])  # Show first 3 rows and 10 features"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze most important terms per category\n",
    "def get_top_tfidf_terms(category, n_terms=10):\n",
    "    \"\"\"\n",
    "    Get top TF-IDF terms for a specific category\n",
    "    \n",
    "    üí° TIP: This function should:\n",
    "    - Filter data for the specific category\n",
    "    - Calculate mean TF-IDF scores for each term\n",
    "    - Return top N terms with highest scores\n",
    "    \"\"\"\n",
    "    # üöÄ YOUR CODE HERE: Implement category-specific TF-IDF analysis\n",
    "    category_data = tfidf_df[tfidf_df['category'] == category]\n",
    "    \n",
    "    # Calculate mean TF-IDF scores for this category (excluding the category column)\n",
    "    mean_scores = category_data.drop('category', axis=1).mean().sort_values(ascending=False)\n",
    "    \n",
    "    return mean_scores.head(n_terms)\n",
    "\n",
    "# Analyze top terms for each category\n",
    "print(\"üè∑Ô∏è TOP TF-IDF TERMS BY CATEGORY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "categories = df['category'].unique()\n",
    "category_terms = {}\n",
    "\n",
    "for category in categories:\n",
    "    top_terms = get_top_tfidf_terms(category, n_terms=10)\n",
    "    category_terms[category] = top_terms\n",
    "    \n",
    "    print(f\"\\nüì∞ {category.upper()}:\")\n",
    "    for term, score in top_terms.items():\n",
    "        print(f\"  {term}: {score:.4f}\")\n",
    "\n",
    "# üí° STUDENT TASK: Create visualizations for TF-IDF analysis\n",
    "# - Word clouds for each category\n",
    "# - Bar charts of top terms\n",
    "# - Heatmap of term importance across categories"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üè∑Ô∏è Part-of-Speech Analysis\n",
    "\n",
    "### üéØ Module 4: Grammatical Pattern Analysis\n",
    "\n",
    "Let's analyze the grammatical patterns in different news categories using Part-of-Speech (POS) tagging. This can reveal interesting differences in writing styles between categories.\n",
    "\n",
    "**POS Analysis Applications:**\n",
    "- **Writing Style Detection:** Different categories may use different grammatical patterns\n",
    "- **Content Quality Assessment:** Proper noun density, adjective usage, etc.\n",
    "- **Feature Engineering:** POS tags can be features for classification\n",
    "\n",
    "**üí° Hypothesis:** Sports articles might have more action verbs, while business articles might have more numbers and proper nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def analyze_pos_patterns(text):\n",
    "    \"\"\"\n",
    "    Analyze POS patterns in text\n",
    "    \n",
    "    üí° TIP: This function should:\n",
    "    - Tokenize the text\n",
    "    - Apply POS tagging\n",
    "    - Count different POS categories\n",
    "    - Return proportions or counts\n",
    "    \"\"\"\n",
    "    if not text or pd.isna(text):\n",
    "        return {}\n",
    "    \n",
    "    # üöÄ YOUR CODE HERE: Implement POS analysis\n",
    "    # Tokenize and tag\n",
    "    tokens = word_tokenize(str(text))\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    \n",
    "    # Count POS categories\n",
    "    pos_counts = Counter([tag for word, tag in pos_tags])\n",
    "    total_words = len(pos_tags)\n",
    "    \n",
    "    if total_words == 0:\n",
    "        return {}\n",
    "    \n",
    "    # Convert to proportions\n",
    "    pos_proportions = {pos: count/total_words for pos, count in pos_counts.items()}\n",
    "    \n",
    "    return pos_proportions\n",
    "\n",
    "# Apply POS analysis to all articles\n",
    "print(\"üè∑Ô∏è Analyzing POS patterns...\")\n",
    "\n",
    "# Analyze POS for each article\n",
    "pos_results = []\n",
    "for idx, row in df.iterrows():\n",
    "    pos_analysis = analyze_pos_patterns(row['full_text'])\n",
    "    pos_analysis['category'] = row['category']\n",
    "    pos_analysis['article_id'] = row['article_id']\n",
    "    pos_results.append(pos_analysis)\n",
    "\n",
    "# Convert to DataFrame\n",
    "pos_df = pd.DataFrame(pos_results).fillna(0)\n",
    "\n",
    "print(f\"‚úÖ POS analysis complete!\")\n",
    "print(f\"üìä Found {len(pos_df.columns)-2} different POS tags\")\n",
    "\n",
    "# Show sample results\n",
    "print(\"\\nüìù Sample POS analysis:\")\n",
    "print(pos_df.head())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze POS patterns by category\n",
    "print(\"üìä POS PATTERNS BY CATEGORY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Group by category and calculate mean proportions\n",
    "pos_by_category = pos_df.groupby('category').mean()\n",
    "\n",
    "# Focus on major POS categories\n",
    "major_pos = ['NN', 'NNS', 'NNP', 'NNPS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', \n",
    "             'JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS', 'CD']\n",
    "\n",
    "# Filter to only include major POS tags that exist in our data\n",
    "available_pos = [pos for pos in major_pos if pos in pos_by_category.columns]\n",
    "\n",
    "if available_pos:\n",
    "    pos_summary = pos_by_category[available_pos]\n",
    "    \n",
    "    print(\"\\nüéØ Key POS patterns by category:\")\n",
    "    print(pos_summary.round(4))\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(pos_summary.T, annot=True, cmap='YlOrRd', fmt='.3f')\n",
    "    plt.title('POS Tag Proportions by News Category')\n",
    "    plt.xlabel('Category')\n",
    "    plt.ylabel('POS Tag')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # üí° STUDENT TASK: Analyze the patterns\n",
    "    # - Which categories use more nouns vs verbs?\n",
    "    # - Do business articles have more numbers (CD)?\n",
    "    # - Are there differences in adjective usage?\n",
    "    \n",
    "    print(\"\\nüí° ANALYSIS QUESTIONS:\")\n",
    "    print(\"1. Which category has the highest proportion of proper nouns (NNP/NNPS)?\")\n",
    "    print(\"2. Which category uses the most action verbs (VB, VBD, VBG)?\")\n",
    "    print(\"3. Are there interesting patterns in adjective (JJ) usage?\")\n",
    "    print(\"4. How does number (CD) usage vary across categories?\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No major POS tags found in the analysis. Check your POS tagging implementation.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üå≥ Syntax Parsing and Semantic Analysis\n",
    "\n",
    "### üéØ Module 5: Understanding Sentence Structure\n",
    "\n",
    "Now we'll use spaCy to perform dependency parsing and extract semantic relationships from our news articles. This helps us understand not just what words are present, but how they relate to each other.\n",
    "\n",
    "**Dependency Parsing Applications:**\n",
    "- **Relationship Extraction:** Find connections between entities\n",
    "- **Event Detection:** Identify who did what to whom\n",
    "- **Information Extraction:** Extract structured facts from unstructured text\n",
    "\n",
    "**üí° Business Value:** Understanding sentence structure helps extract more precise information about events, relationships, and actions mentioned in news articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def extract_syntactic_features(text):\n",
    "    \"\"\"\n",
    "    Extract syntactic features using spaCy dependency parsing\n",
    "    \n",
    "    üí° TIP: This function should extract:\n",
    "    - Dependency relations\n",
    "    - Subject-verb-object patterns\n",
    "    - Noun phrases\n",
    "    - Verb phrases\n",
    "    \"\"\"\n",
    "    if not text or pd.isna(text):\n",
    "        return {}\n",
    "    \n",
    "    # Process text with spaCy\n",
    "    doc = nlp(str(text))\n",
    "    \n",
    "    features = {\n",
    "        'num_sentences': len(list(doc.sents)),\n",
    "        'num_tokens': len(doc),\n",
    "        'dependency_relations': [],\n",
    "        'noun_phrases': [],\n",
    "        'verb_phrases': [],\n",
    "        'subjects': [],\n",
    "        'objects': []\n",
    "    }\n",
    "    \n",
    "    # üöÄ YOUR CODE HERE: Extract syntactic features\n",
    "    \n",
    "    # Extract dependency relations\n",
    "    for token in doc:\n",
    "        if not token.is_space and not token.is_punct:\n",
    "            features['dependency_relations'].append(token.dep_)\n",
    "    \n",
    "    # Extract noun phrases\n",
    "    for chunk in doc.noun_chunks:\n",
    "        features['noun_phrases'].append(chunk.text.lower())\n",
    "    \n",
    "    # Extract subjects and objects\n",
    "    for token in doc:\n",
    "        if token.dep_ in ['nsubj', 'nsubjpass']:  # Subjects\n",
    "            features['subjects'].append(token.text.lower())\n",
    "        elif token.dep_ in ['dobj', 'iobj', 'pobj']:  # Objects\n",
    "            features['objects'].append(token.text.lower())\n",
    "    \n",
    "    # Count dependency types\n",
    "    dep_counts = Counter(features['dependency_relations'])\n",
    "    features['dependency_counts'] = dict(dep_counts)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Apply syntactic analysis to sample articles\n",
    "print(\"üå≥ Performing syntactic analysis...\")\n",
    "\n",
    "# Analyze first few articles (to save computation time)\n",
    "syntactic_results = []\n",
    "for idx, row in df.head(5).iterrows():  # Limit to first 5 for demo\n",
    "    features = extract_syntactic_features(row['full_text'])\n",
    "    features['category'] = row['category']\n",
    "    features['article_id'] = row['article_id']\n",
    "    syntactic_results.append(features)\n",
    "\n",
    "print(\"‚úÖ Syntactic analysis complete!\")\n",
    "\n",
    "# Display results\n",
    "for i, result in enumerate(syntactic_results):\n",
    "    print(f\"\\nüì∞ Article {i+1} ({result['category']}):\")\n",
    "    print(f\"  Sentences: {result['num_sentences']}\")\n",
    "    print(f\"  Tokens: {result['num_tokens']}\")\n",
    "    print(f\"  Noun phrases: {result['noun_phrases'][:3]}...\")  # Show first 3\n",
    "    print(f\"  Subjects: {result['subjects'][:3]}...\")  # Show first 3\n",
    "    print(f\"  Objects: {result['objects'][:3]}...\")  # Show first 3"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize dependency parsing for a sample sentence\n",
    "from spacy import displacy\n",
    "\n",
    "# Choose a sample sentence\n",
    "sample_sentence = df.iloc[0]['content']  # First article's content\n",
    "print(f\"üìù Sample sentence: {sample_sentence}\")\n",
    "\n",
    "# Process with spaCy\n",
    "doc = nlp(sample_sentence)\n",
    "\n",
    "# Display dependency tree (this works best in Jupyter)\n",
    "print(\"\\nüå≥ Dependency Parse Visualization:\")\n",
    "try:\n",
    "    # This will create an interactive visualization in Jupyter\n",
    "    displacy.render(doc, style=\"dep\", jupyter=True)\n",
    "except:\n",
    "    # Fallback: print dependency information\n",
    "    print(\"\\nüîó Dependency Relations:\")\n",
    "    for token in doc:\n",
    "        if not token.is_space and not token.is_punct:\n",
    "            print(f\"  {token.text} --> {token.dep_} --> {token.head.text}\")\n",
    "\n",
    "# üí° STUDENT TASK: Extend syntactic analysis\n",
    "# - Compare syntactic complexity across categories\n",
    "# - Extract action patterns (who did what)\n",
    "# - Identify most common dependency relations per category\n",
    "# - Create features for classification based on syntax"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üòä Sentiment and Emotion Analysis\n",
    "\n",
    "### üéØ Module 6: Understanding Emotional Tone\n",
    "\n",
    "Let's analyze the sentiment and emotional tone of our news articles. This can reveal interesting patterns about how different types of news are presented and perceived.\n",
    "\n",
    "**Sentiment Analysis Applications:**\n",
    "- **Media Bias Detection:** Identify emotional slant in news coverage\n",
    "- **Public Opinion Tracking:** Monitor sentiment trends over time\n",
    "- **Content Recommendation:** Suggest articles based on emotional tone\n",
    "\n",
    "**üí° Hypothesis:** Different news categories might have different emotional profiles - sports might be more positive, politics more negative, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    \"\"\"\n",
    "    Analyze sentiment using VADER sentiment analyzer\n",
    "    \n",
    "    üí° TIP: VADER returns:\n",
    "    - compound: overall sentiment (-1 to 1)\n",
    "    - pos: positive score (0 to 1)\n",
    "    - neu: neutral score (0 to 1)\n",
    "    - neg: negative score (0 to 1)\n",
    "    \"\"\"\n",
    "    if not text or pd.isna(text):\n",
    "        return {'compound': 0, 'pos': 0, 'neu': 1, 'neg': 0}\n",
    "    \n",
    "    # üöÄ YOUR CODE HERE: Implement sentiment analysis\n",
    "    scores = sia.polarity_scores(str(text))\n",
    "    \n",
    "    # Add interpretation\n",
    "    if scores['compound'] >= 0.05:\n",
    "        scores['sentiment_label'] = 'positive'\n",
    "    elif scores['compound'] <= -0.05:\n",
    "        scores['sentiment_label'] = 'negative'\n",
    "    else:\n",
    "        scores['sentiment_label'] = 'neutral'\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Apply sentiment analysis to all articles\n",
    "print(\"üòä Analyzing sentiment...\")\n",
    "\n",
    "sentiment_results = []\n",
    "for idx, row in df.iterrows():\n",
    "    # Analyze both title and content\n",
    "    title_sentiment = analyze_sentiment(row['title'])\n",
    "    content_sentiment = analyze_sentiment(row['content'])\n",
    "    full_sentiment = analyze_sentiment(row['full_text'])\n",
    "    \n",
    "    result = {\n",
    "        'article_id': row['article_id'],\n",
    "        'category': row['category'],\n",
    "        'title_sentiment': title_sentiment['compound'],\n",
    "        'title_label': title_sentiment['sentiment_label'],\n",
    "        'content_sentiment': content_sentiment['compound'],\n",
    "        'content_label': content_sentiment['sentiment_label'],\n",
    "        'full_sentiment': full_sentiment['compound'],\n",
    "        'full_label': full_sentiment['sentiment_label'],\n",
    "        'pos_score': full_sentiment['pos'],\n",
    "        'neu_score': full_sentiment['neu'],\n",
    "        'neg_score': full_sentiment['neg']\n",
    "    }\n",
    "    sentiment_results.append(result)\n",
    "\n",
    "# Convert to DataFrame\n",
    "sentiment_df = pd.DataFrame(sentiment_results)\n",
    "\n",
    "print(\"‚úÖ Sentiment analysis complete!\")\n",
    "print(f\"üìä Analyzed {len(sentiment_df)} articles\")\n",
    "\n",
    "# Display sample results\n",
    "print(\"\\nüìù Sample sentiment results:\")\n",
    "print(sentiment_df[['category', 'full_sentiment', 'full_label']].head())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze sentiment patterns by category\n",
    "print(\"üìä SENTIMENT ANALYSIS BY CATEGORY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Calculate sentiment statistics by category\n",
    "sentiment_by_category = sentiment_df.groupby('category').agg({\n",
    "    'full_sentiment': ['mean', 'std', 'min', 'max'],\n",
    "    'pos_score': 'mean',\n",
    "    'neu_score': 'mean',\n",
    "    'neg_score': 'mean'\n",
    "}).round(4)\n",
    "\n",
    "print(\"\\nüìà Sentiment statistics by category:\")\n",
    "print(sentiment_by_category)\n",
    "\n",
    "# Sentiment distribution by category\n",
    "sentiment_dist = sentiment_df.groupby(['category', 'full_label']).size().unstack(fill_value=0)\n",
    "sentiment_dist_pct = sentiment_dist.div(sentiment_dist.sum(axis=1), axis=0) * 100\n",
    "\n",
    "print(\"\\nüìä Sentiment distribution (%) by category:\")\n",
    "print(sentiment_dist_pct.round(2))\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Sentiment scores by category\n",
    "sns.boxplot(data=sentiment_df, x='category', y='full_sentiment', ax=axes[0,0])\n",
    "axes[0,0].set_title('Sentiment Score Distribution by Category')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. Sentiment label distribution\n",
    "sentiment_dist_pct.plot(kind='bar', ax=axes[0,1], stacked=True)\n",
    "axes[0,1].set_title('Sentiment Label Distribution by Category (%)')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "axes[0,1].legend(title='Sentiment')\n",
    "\n",
    "# 3. Positive vs Negative scores\n",
    "category_means = sentiment_df.groupby('category')[['pos_score', 'neg_score']].mean()\n",
    "category_means.plot(kind='bar', ax=axes[1,0])\n",
    "axes[1,0].set_title('Average Positive vs Negative Scores by Category')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "axes[1,0].legend(['Positive', 'Negative'])\n",
    "\n",
    "# 4. Sentiment vs Category heatmap\n",
    "sentiment_pivot = sentiment_df.pivot_table(values='full_sentiment', index='category', \n",
    "                                         columns='full_label', aggfunc='count', fill_value=0)\n",
    "sns.heatmap(sentiment_pivot, annot=True, fmt='d', ax=axes[1,1], cmap='YlOrRd')\n",
    "axes[1,1].set_title('Sentiment Count Heatmap')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# üí° STUDENT TASK: Analyze sentiment patterns\n",
    "# - Which categories are most positive/negative?\n",
    "# - Are there differences between title and content sentiment?\n",
    "# - How does sentiment vary within categories?\n",
    "# - Can sentiment be used as a feature for classification?"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üè∑Ô∏è Text Classification System\n",
    "\n",
    "### üéØ Module 7: Building the News Classifier\n",
    "\n",
    "Now we'll build the core of our NewsBot system - a multi-class text classifier that can automatically categorize news articles. We'll compare different algorithms and evaluate their performance.\n",
    "\n",
    "**Classification Pipeline:**\n",
    "1. **Feature Engineering:** Combine TF-IDF with other features\n",
    "2. **Model Training:** Train multiple algorithms\n",
    "3. **Model Evaluation:** Compare performance metrics\n",
    "4. **Model Selection:** Choose the best performing model\n",
    "\n",
    "**üí° Business Impact:** Accurate classification enables automatic content routing, personalized recommendations, and efficient content management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Prepare features for classification\n",
    "print(\"üîß Preparing features for classification...\")\n",
    "\n",
    "# üí° TIP: Combine multiple feature types for better performance\n",
    "# - TF-IDF features (most important)\n",
    "# - Sentiment features\n",
    "# - Text length features\n",
    "# - POS features (if available)\n",
    "\n",
    "# Create feature matrix\n",
    "X_tfidf = tfidf_matrix.toarray()  # TF-IDF features\n",
    "\n",
    "# Add sentiment features\n",
    "sentiment_features = sentiment_df[['full_sentiment', 'pos_score', 'neu_score', 'neg_score']].values\n",
    "\n",
    "# Add text length features\n",
    "length_features = np.array([\n",
    "    df['full_text'].str.len(),  # Character length\n",
    "    df['full_text'].str.split().str.len(),  # Word count\n",
    "    df['title'].str.len(),  # Title length\n",
    "]).T\n",
    "\n",
    "# üöÄ YOUR CODE HERE: Combine all features\n",
    "X_combined = np.hstack([\n",
    "    X_tfidf,\n",
    "    sentiment_features,\n",
    "    length_features\n",
    "])\n",
    "\n",
    "# Target variable\n",
    "y = df['category'].values\n",
    "\n",
    "print(f\"‚úÖ Feature matrix prepared!\")\n",
    "print(f\"üìä Feature matrix shape: {X_combined.shape}\")\n",
    "print(f\"üéØ Number of classes: {len(np.unique(y))}\")\n",
    "print(f\"üìã Classes: {np.unique(y)}\")\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_combined, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nüìà Data split:\")\n",
    "print(f\"  Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"  Test set: {X_test.shape[0]} samples\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train and evaluate multiple classifiers\n",
    "print(\"ü§ñ Training multiple classifiers...\")\n",
    "\n",
    "# Define classifiers to compare\n",
    "classifiers = {\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'SVM': SVC(random_state=42, probability=True)  # Enable probability for better analysis\n",
    "}\n",
    "\n",
    "# üí° TIP: For larger datasets, you might want to use SGDClassifier for efficiency\n",
    "# from sklearn.linear_model import SGDClassifier\n",
    "# classifiers['SGD'] = SGDClassifier(random_state=42)\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "results = {}\n",
    "trained_models = {}\n",
    "\n",
    "for name, classifier in classifiers.items():\n",
    "    print(f\"\\nüîÑ Training {name}...\")\n",
    "    \n",
    "    # üöÄ YOUR CODE HERE: Train and evaluate classifier\n",
    "    # Train the model\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    y_pred_proba = classifier.predict_proba(X_test) if hasattr(classifier, 'predict_proba') else None\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(classifier, X_train, y_train, cv=3, scoring='accuracy')\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    trained_models[name] = classifier\n",
    "    \n",
    "    print(f\"  ‚úÖ Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  üìä CV Score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "\n",
    "print(\"\\nüèÜ CLASSIFIER COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Test Accuracy': [results[name]['accuracy'] for name in results.keys()],\n",
    "    'CV Mean': [results[name]['cv_mean'] for name in results.keys()],\n",
    "    'CV Std': [results[name]['cv_std'] for name in results.keys()]\n",
    "})\n",
    "\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "# Find best model\n",
    "best_model_name = comparison_df.loc[comparison_df['Test Accuracy'].idxmax(), 'Model']\n",
    "print(f\"\\nü•á Best performing model: {best_model_name}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Detailed evaluation of the best model\n",
    "best_model = trained_models[best_model_name]\n",
    "best_predictions = results[best_model_name]['predictions']\n",
    "\n",
    "print(f\"üìä DETAILED EVALUATION: {best_model_name}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nüìã Classification Report:\")\n",
    "print(classification_report(y_test, best_predictions))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, best_predictions)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=np.unique(y), yticklabels=np.unique(y))\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature importance (for Logistic Regression)\n",
    "if best_model_name == 'Logistic Regression':\n",
    "    print(\"\\nüîç Top Features by Category:\")\n",
    "    feature_names_extended = list(feature_names) + ['sentiment', 'pos_score', 'neu_score', 'neg_score', \n",
    "                                                   'char_length', 'word_count', 'title_length']\n",
    "    \n",
    "    classes = best_model.classes_\n",
    "    coefficients = best_model.coef_\n",
    "    \n",
    "    for i, class_name in enumerate(classes):\n",
    "        top_indices = np.argsort(coefficients[i])[-10:]  # Top 10 features\n",
    "        print(f\"\\nüì∞ {class_name}:\")\n",
    "        for idx in reversed(top_indices):\n",
    "            if idx < len(feature_names_extended):\n",
    "                print(f\"  {feature_names_extended[idx]}: {coefficients[i][idx]:.4f}\")\n",
    "\n",
    "# üí° STUDENT TASK: Improve the classifier\n",
    "# - Try different feature combinations\n",
    "# - Experiment with hyperparameter tuning\n",
    "# - Add more sophisticated features\n",
    "# - Handle class imbalance if present"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Named Entity Recognition\n",
    "\n",
    "### üéØ Module 8: Extracting Facts from News\n",
    "\n",
    "Now we'll implement Named Entity Recognition to extract specific facts from our news articles. This transforms unstructured text into structured, queryable information.\n",
    "\n",
    "**NER Applications:**\n",
    "- **Entity Tracking:** Monitor mentions of people, organizations, locations\n",
    "- **Fact Extraction:** Build knowledge bases from news content\n",
    "- **Relationship Mapping:** Understand connections between entities\n",
    "- **Timeline Construction:** Track events and their participants\n",
    "\n",
    "**üí° Business Value:** NER enables sophisticated analysis like \"Show me all articles mentioning Apple Inc. and their financial performance\" or \"Track mentions of political figures over time.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def extract_entities(text):\n",
    "    \"\"\"\n",
    "    Extract named entities using spaCy\n",
    "    \n",
    "    üí° TIP: spaCy recognizes these entity types:\n",
    "    - PERSON: People, including fictional\n",
    "    - ORG: Companies, agencies, institutions\n",
    "    - GPE: Countries, cities, states\n",
    "    - MONEY: Monetary values\n",
    "    - DATE: Absolute or relative dates\n",
    "    - TIME: Times smaller than a day\n",
    "    - And many more...\n",
    "    \"\"\"\n",
    "    if not text or pd.isna(text):\n",
    "        return []\n",
    "    \n",
    "    # üöÄ YOUR CODE HERE: Implement entity extraction\n",
    "    doc = nlp(str(text))\n",
    "    \n",
    "    entities = []\n",
    "    for ent in doc.ents:\n",
    "        entities.append({\n",
    "            'text': ent.text,\n",
    "            'label': ent.label_,\n",
    "            'start': ent.start_char,\n",
    "            'end': ent.end_char,\n",
    "            'description': spacy.explain(ent.label_)\n",
    "        })\n",
    "    \n",
    "    return entities\n",
    "\n",
    "# Apply NER to all articles\n",
    "print(\"üîç Extracting named entities...\")\n",
    "\n",
    "all_entities = []\n",
    "article_entities = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    entities = extract_entities(row['full_text'])\n",
    "    \n",
    "    # Store entities for this article\n",
    "    article_entities.append({\n",
    "        'article_id': row['article_id'],\n",
    "        'category': row['category'],\n",
    "        'entities': entities,\n",
    "        'entity_count': len(entities)\n",
    "    })\n",
    "    \n",
    "    # Add to global entity list\n",
    "    for entity in entities:\n",
    "        entity['article_id'] = row['article_id']\n",
    "        entity['category'] = row['category']\n",
    "        all_entities.append(entity)\n",
    "\n",
    "print(f\"‚úÖ Entity extraction complete!\")\n",
    "print(f\"üìä Total entities found: {len(all_entities)}\")\n",
    "print(f\"üì∞ Articles processed: {len(article_entities)}\")\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "entities_df = pd.DataFrame(all_entities)\n",
    "\n",
    "if not entities_df.empty:\n",
    "    print(f\"\\nüè∑Ô∏è Entity types found: {entities_df['label'].unique()}\")\n",
    "    print(\"\\nüìù Sample entities:\")\n",
    "    print(entities_df[['text', 'label', 'category']].head(10))\nelse:\n",
    "    print(\"‚ö†Ô∏è No entities found. This might happen with very short sample texts.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze entity patterns\n",
    "if not entities_df.empty:\n",
    "    print(\"üìä NAMED ENTITY ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Entity type distribution\n",
    "    entity_counts = entities_df['label'].value_counts()\n",
    "    print(\"\\nüè∑Ô∏è Entity type distribution:\")\n",
    "    print(entity_counts)\n",
    "    \n",
    "    # Entity types by category\n",
    "    entity_by_category = entities_df.groupby(['category', 'label']).size().unstack(fill_value=0)\n",
    "    print(\"\\nüì∞ Entity types by news category:\")\n",
    "    print(entity_by_category)\n",
    "    \n",
    "    # Most frequent entities\n",
    "    print(\"\\nüî• Most frequent entities:\")\n",
    "    frequent_entities = entities_df.groupby(['text', 'label']).size().sort_values(ascending=False).head(15)\n",
    "    for (entity, label), count in frequent_entities.items():\n",
    "        print(f\"  {entity} ({label}): {count} mentions\")\n",
    "    \n",
    "    # Visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Entity type distribution\n",
    "    entity_counts.plot(kind='bar', ax=axes[0,0])\n",
    "    axes[0,0].set_title('Entity Type Distribution')\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 2. Entities per category\n",
    "    entities_per_category = entities_df.groupby('category').size()\n",
    "    entities_per_category.plot(kind='bar', ax=axes[0,1])\n",
    "    axes[0,1].set_title('Total Entities per Category')\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 3. Entity type heatmap by category\n",
    "    if entity_by_category.shape[0] > 1 and entity_by_category.shape[1] > 1:\n",
    "        sns.heatmap(entity_by_category, annot=True, fmt='d', ax=axes[1,0], cmap='YlOrRd')\n",
    "        axes[1,0].set_title('Entity Types by Category Heatmap')\n",
    "    else:\n",
    "        axes[1,0].text(0.5, 0.5, 'Insufficient data\\nfor heatmap', \n",
    "                      ha='center', va='center', transform=axes[1,0].transAxes)\n",
    "        axes[1,0].set_title('Entity Types by Category')\n",
    "    \n",
    "    # 4. Top entities\n",
    "    top_entities = entities_df['text'].value_counts().head(10)\n",
    "    top_entities.plot(kind='barh', ax=axes[1,1])\n",
    "    axes[1,1].set_title('Most Mentioned Entities')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # üí° STUDENT TASK: Advanced entity analysis\n",
    "    # - Create entity co-occurrence networks\n",
    "    # - Track entity mentions over time\n",
    "    # - Build entity relationship graphs\n",
    "    # - Identify entity sentiment associations\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping entity analysis due to insufficient data.\")\n",
    "    print(\"üí° TIP: Try with a larger, more diverse dataset for better NER results.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Comprehensive Analysis and Insights\n",
    "\n",
    "### üéØ Bringing It All Together\n",
    "\n",
    "Now let's combine all our analyses to generate comprehensive insights about our news dataset. This is where the real business value emerges from our NLP pipeline.\n",
    "\n",
    "**Key Analysis Areas:**\n",
    "1. **Cross-Category Patterns:** How do different news types differ linguistically?\n",
    "2. **Entity-Sentiment Relationships:** What entities are associated with positive/negative coverage?\n",
    "3. **Content Quality Metrics:** Which categories have the most informative content?\n",
    "4. **Classification Performance:** How well can we automatically categorize news?\n",
    "\n",
    "**üí° Business Applications:** These insights can inform content strategy, editorial decisions, and automated content management systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create comprehensive analysis dashboard\n",
    "def create_comprehensive_analysis():\n",
    "    \"\"\"\n",
    "    Generate comprehensive insights combining all analyses\n",
    "    \n",
    "    üí° TIP: This function should combine:\n",
    "    - Classification performance\n",
    "    - Sentiment patterns\n",
    "    - Entity distributions\n",
    "    - Linguistic features\n",
    "    \"\"\"\n",
    "    \n",
    "    insights = {\n",
    "        'dataset_overview': {},\n",
    "        'classification_performance': {},\n",
    "        'sentiment_insights': {},\n",
    "        'entity_insights': {},\n",
    "        'linguistic_patterns': {},\n",
    "        'business_recommendations': []\n",
    "    }\n",
    "    \n",
    "    # üöÄ YOUR CODE HERE: Generate comprehensive insights\n",
    "    \n",
    "    # Dataset overview\n",
    "    insights['dataset_overview'] = {\n",
    "        'total_articles': len(df),\n",
    "        'categories': df['category'].unique().tolist(),\n",
    "        'category_distribution': df['category'].value_counts().to_dict(),\n",
    "        'avg_article_length': df['full_text'].str.len().mean(),\n",
    "        'avg_words_per_article': df['full_text'].str.split().str.len().mean()\n",
    "    }\n",
    "    \n",
    "    # Classification performance\n",
    "    insights['classification_performance'] = {\n",
    "        'best_model': best_model_name,\n",
    "        'best_accuracy': results[best_model_name]['accuracy'],\n",
    "        'model_comparison': {name: results[name]['accuracy'] for name in results.keys()}\n",
    "    }\n",
    "    \n",
    "    # Sentiment insights\n",
    "    sentiment_by_cat = sentiment_df.groupby('category')['full_sentiment'].mean().to_dict()\n",
    "    insights['sentiment_insights'] = {\n",
    "        'most_positive_category': max(sentiment_by_cat, key=sentiment_by_cat.get),\n",
    "        'most_negative_category': min(sentiment_by_cat, key=sentiment_by_cat.get),\n",
    "        'sentiment_by_category': sentiment_by_cat,\n",
    "        'overall_sentiment': sentiment_df['full_sentiment'].mean()\n",
    "    }\n",
    "    \n",
    "    # Entity insights\n",
    "    if not entities_df.empty:\n",
    "        entity_by_cat = entities_df.groupby('category').size().to_dict()\n",
    "        insights['entity_insights'] = {\n",
    "            'total_entities': len(entities_df),\n",
    "            'unique_entities': entities_df['text'].nunique(),\n",
    "            'entity_types': entities_df['label'].unique().tolist(),\n",
    "            'entities_per_category': entity_by_cat,\n",
    "            'most_mentioned_entities': entities_df['text'].value_counts().head(5).to_dict()\n",
    "        }\n",
    "    \n",
    "    # Generate business recommendations\n",
    "    recommendations = []\n",
    "    \n",
    "    # Classification recommendations\n",
    "    if insights['classification_performance']['best_accuracy'] > 0.8:\n",
    "        recommendations.append(\"‚úÖ High classification accuracy achieved - ready for automated content routing\")\n",
    "    else:\n",
    "        recommendations.append(\"‚ö†Ô∏è Classification accuracy needs improvement - consider more training data or feature engineering\")\n",
    "    \n",
    "    # Sentiment recommendations\n",
    "    pos_cat = insights['sentiment_insights']['most_positive_category']\n",
    "    neg_cat = insights['sentiment_insights']['most_negative_category']\n",
    "    recommendations.append(f\"üìä {pos_cat} articles are most positive - good for uplifting content recommendations\")\n",
    "    recommendations.append(f\"üìä {neg_cat} articles are most negative - may need balanced coverage monitoring\")\n",
    "    \n",
    "    # Entity recommendations\n",
    "    if 'entity_insights' in insights and insights['entity_insights']:\n",
    "        recommendations.append(\"üîç Rich entity extraction enables advanced search and relationship analysis\")\n",
    "    \n",
    "    insights['business_recommendations'] = recommendations\n",
    "    \n",
    "    return insights\n",
    "\n",
    "# Generate comprehensive analysis\n",
    "print(\"üìä Generating comprehensive analysis...\")\n",
    "analysis_results = create_comprehensive_analysis()\n",
    "\n",
    "print(\"‚úÖ Analysis complete!\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìà NEWSBOT INTELLIGENCE SYSTEM - COMPREHENSIVE REPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Display key insights\n",
    "print(f\"\\nüìä DATASET OVERVIEW:\")\n",
    "overview = analysis_results['dataset_overview']\n",
    "print(f\"  Total Articles: {overview['total_articles']}\")\n",
    "print(f\"  Categories: {', '.join(overview['categories'])}\")\n",
    "print(f\"  Average Article Length: {overview['avg_article_length']:.0f} characters\")\n",
    "print(f\"  Average Words per Article: {overview['avg_words_per_article']:.0f} words\")\n",
    "\n",
    "print(f\"\\nü§ñ CLASSIFICATION PERFORMANCE:\")\n",
    "perf = analysis_results['classification_performance']\n",
    "print(f\"  Best Model: {perf['best_model']}\")\n",
    "print(f\"  Best Accuracy: {perf['best_accuracy']:.4f}\")\n",
    "\n",
    "print(f\"\\nüòä SENTIMENT INSIGHTS:\")\n",
    "sent = analysis_results['sentiment_insights']\n",
    "print(f\"  Most Positive Category: {sent['most_positive_category']}\")\n",
    "print(f\"  Most Negative Category: {sent['most_negative_category']}\")\n",
    "print(f\"  Overall Sentiment: {sent['overall_sentiment']:.4f}\")\n",
    "\n",
    "if 'entity_insights' in analysis_results and analysis_results['entity_insights']:\n",
    "    print(f\"\\nüîç ENTITY INSIGHTS:\")\n",
    "    ent = analysis_results['entity_insights']\n",
    "    print(f\"  Total Entities: {ent['total_entities']}\")\n",
    "    print(f\"  Unique Entities: {ent['unique_entities']}\")\n",
    "    print(f\"  Entity Types: {', '.join(ent['entity_types'])}\")\n",
    "\n",
    "print(f\"\\nüí° BUSINESS RECOMMENDATIONS:\")\n",
    "for i, rec in enumerate(analysis_results['business_recommendations'], 1):\n",
    "    print(f\"  {i}. {rec}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Final System Integration\n",
    "\n",
    "### üéØ Building the Complete NewsBot Pipeline\n",
    "\n",
    "Let's create a complete, integrated system that can process new articles from start to finish. This demonstrates the real-world application of all the techniques we've learned.\n",
    "\n",
    "**Complete Pipeline:**\n",
    "1. **Text Preprocessing:** Clean and normalize input\n",
    "2. **Feature Extraction:** Generate TF-IDF and other features\n",
    "3. **Classification:** Predict article category\n",
    "4. **Entity Extraction:** Identify key facts\n",
    "5. **Sentiment Analysis:** Determine emotional tone\n",
    "6. **Insight Generation:** Provide actionable intelligence\n",
    "\n",
    "**üí° Production Ready:** This pipeline can be deployed as a web service, batch processor, or integrated into content management systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class NewsBotIntelligenceSystem:\n",
    "    \"\"\"\n",
    "    Complete NewsBot Intelligence System\n",
    "    \n",
    "    üí° TIP: This class should encapsulate:\n",
    "    - All preprocessing functions\n",
    "    - Trained classification model\n",
    "    - Entity extraction pipeline\n",
    "    - Sentiment analysis\n",
    "    - Insight generation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, classifier, vectorizer, sentiment_analyzer):\n",
    "        self.classifier = classifier\n",
    "        self.vectorizer = vectorizer\n",
    "        self.sentiment_analyzer = sentiment_analyzer\n",
    "        self.nlp = nlp  # spaCy model\n",
    "    \n",
    "    def preprocess_article(self, title, content):\n",
    "        \"\"\"Preprocess a new article\"\"\"\n",
    "        full_text = f\"{title} {content}\"\n",
    "        processed_text = preprocess_text(full_text)\n",
    "        return full_text, processed_text\n",
    "    \n",
    "    def classify_article(self, processed_text):\n",
    "        \"\"\"Classify article category\"\"\"\n",
    "        # üöÄ YOUR CODE HERE: Implement classification\n",
    "        # Transform text to features\n",
    "        features = self.vectorizer.transform([processed_text])\n",
    "        \n",
    "        # Add dummy features for sentiment and length (in production, calculate these)\n",
    "        dummy_features = np.zeros((1, 7))  # 4 sentiment + 3 length features\n",
    "        features_combined = np.hstack([features.toarray(), dummy_features])\n",
    "        \n",
    "        # Predict category and probability\n",
    "        prediction = self.classifier.predict(features_combined)[0]\n",
    "        probabilities = self.classifier.predict_proba(features_combined)[0]\n",
    "        \n",
    "        # Get class probabilities\n",
    "        class_probs = dict(zip(self.classifier.classes_, probabilities))\n",
    "        \n",
    "        return prediction, class_probs\n",
    "    \n",
    "    def extract_entities(self, text):\n",
    "        \"\"\"Extract named entities\"\"\"\n",
    "        return extract_entities(text)\n",
    "    \n",
    "    def analyze_sentiment(self, text):\n",
    "        \"\"\"Analyze sentiment\"\"\"\n",
    "        return analyze_sentiment(text)\n",
    "    \n",
    "    def process_article(self, title, content):\n",
    "        \"\"\"\n",
    "        Complete article processing pipeline\n",
    "        \n",
    "        üí° TIP: This should return a comprehensive analysis including:\n",
    "        - Predicted category with confidence\n",
    "        - Extracted entities\n",
    "        - Sentiment analysis\n",
    "        - Key insights and recommendations\n",
    "        \"\"\"\n",
    "        # üöÄ YOUR CODE HERE: Implement complete pipeline\n",
    "        \n",
    "        # Step 1: Preprocess\n",
    "        full_text, processed_text = self.preprocess_article(title, content)\n",
    "        \n",
    "        # Step 2: Classify\n",
    "        category, category_probs = self.classify_article(processed_text)\n",
    "        \n",
    "        # Step 3: Extract entities\n",
    "        entities = self.extract_entities(full_text)\n",
    "        \n",
    "        # Step 4: Analyze sentiment\n",
    "        sentiment = self.analyze_sentiment(full_text)\n",
    "        \n",
    "        # Step 5: Generate insights\n",
    "        insights = self.generate_insights(category, entities, sentiment, category_probs)\n",
    "        \n",
    "        return {\n",
    "            'title': title,\n",
    "            'content': content[:200] + '...' if len(content) > 200 else content,\n",
    "            'predicted_category': category,\n",
    "            'category_confidence': max(category_probs.values()),\n",
    "            'category_probabilities': category_probs,\n",
    "            'entities': entities,\n",
    "            'sentiment': sentiment,\n",
    "            'insights': insights\n",
    "        }\n",
    "    \n",
    "    def generate_insights(self, category, entities, sentiment, category_probs):\n",
    "        \"\"\"Generate actionable insights\"\"\"\n",
    "        insights = []\n",
    "        \n",
    "        # Classification insights\n",
    "        confidence = max(category_probs.values())\n",
    "        if confidence > 0.8:\n",
    "            insights.append(f\"‚úÖ High confidence {category} classification ({confidence:.2%})\")\n",
    "        else:\n",
    "            insights.append(f\"‚ö†Ô∏è Uncertain classification - consider manual review\")\n",
    "        \n",
    "        # Sentiment insights\n",
    "        if sentiment['compound'] > 0.1:\n",
    "            insights.append(f\"üòä Positive sentiment detected ({sentiment['compound']:.3f})\")\n",
    "        elif sentiment['compound'] < -0.1:\n",
    "            insights.append(f\"üòû Negative sentiment detected ({sentiment['compound']:.3f})\")\n",
    "        else:\n",
    "            insights.append(f\"üòê Neutral sentiment ({sentiment['compound']:.3f})\")\n",
    "        \n",
    "        # Entity insights\n",
    "        if entities:\n",
    "            entity_types = set([e['label'] for e in entities])\n",
    "            insights.append(f\"üîç Found {len(entities)} entities of {len(entity_types)} types\")\n",
    "            \n",
    "            # Highlight important entities\n",
    "            important_entities = [e for e in entities if e['label'] in ['PERSON', 'ORG', 'GPE']]\n",
    "            if important_entities:\n",
    "                key_entities = [e['text'] for e in important_entities[:3]]\n",
    "                insights.append(f\"üéØ Key entities: {', '.join(key_entities)}\")\n",
    "        else:\n",
    "            insights.append(\"‚ÑπÔ∏è No named entities detected\")\n",
    "        \n",
    "        return insights\n",
    "\n",
    "# Initialize the complete system\n",
    "newsbot = NewsBotIntelligenceSystem(\n",
    "    classifier=best_model,\n",
    "    vectorizer=tfidf_vectorizer,\n",
    "    sentiment_analyzer=sia\n",
    ")\n",
    "\n",
    "print(\"ü§ñ NewsBot Intelligence System initialized!\")\n",
    "print(\"‚úÖ Ready to process new articles\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test the complete system with new articles\n",
    "print(\"üß™ TESTING NEWSBOT INTELLIGENCE SYSTEM\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test articles (you can modify these or add your own)\n",
    "test_articles = [\n",
    "    {\n",
    "        'title': 'Microsoft Acquires AI Startup for $2 Billion',\n",
    "        'content': 'Microsoft Corporation announced today the acquisition of an artificial intelligence startup for $2 billion. CEO Satya Nadella said the deal will strengthen Microsoft\\'s position in the AI market and enhance their cloud computing services.'\n",
    "    },\n",
    "    {\n",
    "        'title': 'Lakers Win Championship in Overtime Thriller',\n",
    "        'content': 'The Los Angeles Lakers defeated the Boston Celtics 108-102 in overtime to win the NBA championship. LeBron James scored 35 points and was named Finals MVP for the fourth time in his career.'\n",
    "    },\n",
    "    {\n",
    "        'title': 'New Climate Change Report Shows Alarming Trends',\n",
    "        'content': 'Scientists at the United Nations released a comprehensive climate report showing accelerating global warming. The report warns that immediate action is needed to prevent catastrophic environmental changes.'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Process each test article\n",
    "for i, article in enumerate(test_articles, 1):\n",
    "    print(f\"\\nüì∞ TEST ARTICLE {i}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Process the article\n",
    "    result = newsbot.process_article(article['title'], article['content'])\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"üì∞ Title: {result['title']}\")\n",
    "    print(f\"üìù Content: {result['content']}\")\n",
    "    print(f\"\\nüè∑Ô∏è Predicted Category: {result['predicted_category']} ({result['category_confidence']:.2%} confidence)\")\n",
    "    \n",
    "    print(f\"\\nüìä Category Probabilities:\")\n",
    "    for cat, prob in sorted(result['category_probabilities'].items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  {cat}: {prob:.3f}\")\n",
    "    \n",
    "    print(f\"\\nüòä Sentiment: {result['sentiment']['sentiment_label']} (score: {result['sentiment']['compound']:.3f})\")\n",
    "    \n",
    "    if result['entities']:\n",
    "        print(f\"\\nüîç Entities Found ({len(result['entities'])}):\")\n",
    "        for entity in result['entities'][:5]:  # Show first 5\n",
    "            print(f\"  {entity['text']} ({entity['label']}) - {entity['description']}\")\n",
    "    else:\n",
    "        print(f\"\\nüîç No entities detected\")\n",
    "    \n",
    "    print(f\"\\nüí° Insights:\")\n",
    "    for insight in result['insights']:\n",
    "        print(f\"  {insight}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ NewsBot Intelligence System testing complete!\")\n",
    "print(\"‚úÖ System successfully processed all test articles\")\n",
    "\n",
    "# üí° STUDENT TASK: Test with your own articles\n",
    "# - Try articles from different categories\n",
    "# - Test with articles that might be ambiguous\n",
    "# - Analyze the system's strengths and weaknesses\n",
    "# - Consider how to improve performance"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Project Summary and Next Steps\n",
    "\n",
    "### üéØ What You've Accomplished\n",
    "\n",
    "Congratulations! You've successfully built a comprehensive NewsBot Intelligence System that demonstrates mastery of all NLP techniques covered in Modules 1-8. Let's review what you've achieved:\n",
    "\n",
    "### ‚úÖ Module Integration Checklist\n",
    "- [x] **Module 1:** Applied NLP to real-world news intelligence\n",
    "- [x] **Module 2:** Implemented comprehensive text preprocessing\n",
    "- [x] **Module 3:** Used TF-IDF for feature extraction and analysis\n",
    "- [x] **Module 4:** Analyzed grammatical patterns with POS tagging\n",
    "- [x] **Module 5:** Extracted syntactic relationships with dependency parsing\n",
    "- [x] **Module 6:** Performed sentiment and emotion analysis\n",
    "- [x] **Module 7:** Built and evaluated text classification models\n",
    "- [x] **Module 8:** Implemented Named Entity Recognition\n",
    "\n",
    "### üöÄ System Capabilities\n",
    "Your NewsBot can now:\n",
    "- Automatically categorize news articles with high accuracy\n",
    "- Extract key entities (people, organizations, locations, dates, money)\n",
    "- Analyze sentiment and emotional tone\n",
    "- Identify linguistic patterns and writing styles\n",
    "- Generate actionable business insights\n",
    "- Process new articles through a complete pipeline\n",
    "\n",
    "### üíº Business Value\n",
    "This system provides real business value for:\n",
    "- **Media Companies:** Automated content categorization and routing\n",
    "- **Market Research:** Sentiment tracking and entity monitoring\n",
    "- **Content Management:** Intelligent organization and search\n",
    "- **Business Intelligence:** Trend analysis and competitive monitoring\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Final Deliverables Checklist\n",
    "\n",
    "Before submitting your project, ensure you have:\n",
    "\n",
    "### üìÅ Code and Documentation\n",
    "- [ ] Complete Jupyter notebook with all analyses\n",
    "- [ ] Well-documented functions with docstrings\n",
    "- [ ] Clear markdown explanations for each section\n",
    "- [ ] Organized GitHub repository structure\n",
    "- [ ] README.md with project overview and setup instructions\n",
    "\n",
    "### üìä Analysis and Results\n",
    "- [ ] Comprehensive dataset exploration\n",
    "- [ ] TF-IDF analysis with category-specific insights\n",
    "- [ ] POS tagging patterns across categories\n",
    "- [ ] Syntactic analysis with dependency parsing\n",
    "- [ ] Sentiment analysis with category comparisons\n",
    "- [ ] Classification model comparison and evaluation\n",
    "- [ ] Named Entity Recognition with relationship mapping\n",
    "- [ ] Integrated system demonstration\n",
    "\n",
    "### üìà Visualizations\n",
    "- [ ] Category distribution plots\n",
    "- [ ] TF-IDF word clouds or bar charts\n",
    "- [ ] POS pattern heatmaps\n",
    "- [ ] Sentiment distribution by category\n",
    "- [ ] Confusion matrix for classification\n",
    "- [ ] Entity type and frequency visualizations\n",
    "\n",
    "### üé• Presentation Materials\n",
    "- [ ] 5-7 minute video demonstration\n",
    "- [ ] Written report (3-4 pages)\n",
    "- [ ] Individual reflection papers\n",
    "- [ ] Business recommendations and insights\n",
    "\n",
    "---\n",
    "\n",
    "## üîÆ Future Enhancements\n",
    "\n",
    "Consider these improvements for your portfolio or future projects:\n",
    "\n",
    "### ü§ñ Technical Improvements\n",
    "- **Deep Learning Models:** Implement BERT or other transformer models\n",
    "- **Custom NER:** Train domain-specific entity recognition\n",
    "- **Real-time Processing:** Build streaming data pipeline\n",
    "- **Multi-language Support:** Extend to non-English news\n",
    "\n",
    "### üìä Advanced Analytics\n",
    "- **Topic Modeling:** Discover hidden themes (Module 9 preview!)\n",
    "- **Trend Analysis:** Track entities and sentiment over time\n",
    "- **Network Analysis:** Map entity relationships and co-occurrences\n",
    "- **Bias Detection:** Identify potential media bias patterns\n",
    "\n",
    "### üåê Deployment Options\n",
    "- **Web Application:** Create interactive dashboard with Streamlit\n",
    "- **API Service:** Deploy as REST API for integration\n",
    "- **Mobile App:** Build mobile interface for news analysis\n",
    "- **Browser Extension:** Real-time news analysis while browsing\n",
    "\n",
    "---\n",
    "\n",
    "## üéì Reflection Questions\n",
    "\n",
    "For your individual reflection paper, consider these questions:\n",
    "\n",
    "1. **Technical Mastery:** Which NLP techniques did you find most challenging? Most useful?\n",
    "2. **Integration Challenges:** How did you handle combining multiple NLP tasks?\n",
    "3. **Business Applications:** What real-world problems could this system solve?\n",
    "4. **Ethical Considerations:** What are the potential risks of automated news analysis?\n",
    "5. **Future Learning:** What NLP topics are you most excited to explore next?\n",
    "6. **Team Collaboration:** How did you divide work and ensure quality?\n",
    "7. **Portfolio Value:** How will you present this project to potential employers?\n",
    "\n",
    "---\n",
    "\n",
    "## üèÜ Congratulations!\n",
    "\n",
    "You've successfully completed a comprehensive NLP project that demonstrates real-world application of multiple advanced techniques. This NewsBot Intelligence System is a valuable addition to your portfolio and showcases your ability to:\n",
    "\n",
    "- **Integrate multiple NLP techniques** into a cohesive system\n",
    "- **Handle real-world data** with all its messiness and challenges\n",
    "- **Generate business value** from unstructured text data\n",
    "- **Build production-ready systems** with proper evaluation and monitoring\n",
    "- **Communicate technical results** to both technical and business audiences\n",
    "\n",
    "**üöÄ You're now ready for Module 9: Topic Modeling and Advanced Text Analysis!**\n",
    "\n",
    "---\n",
    "\n",
    "*Remember: The goal isn't just to complete the assignment, but to build something you're proud to show in job interviews and professional discussions. This project demonstrates your practical NLP skills and ability to solve real business problems with AI.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

